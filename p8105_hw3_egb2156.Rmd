---
title: "Homework 3"
author: Emily Bamforth
date: 10 October 2020
output: github_document
---

```{r set_up}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

Load in the data.

```{r load_data}
data("instacart")
```

This dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns.

Each observation in the dataset is based on the `order_id`. There are 15 variables in this dataset, which tell us about the order, the user, and the items, including `order_id`, `user_id`, `product_name`, and `order_hour_of_day`. Location of items are provided by department, aisle and item name.

Information about aisles: how many are there, and from which aisles are the most items purchased?

```{r aisles}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are 134 aisles in this dataset. The most items are purchased from the `fresh vegetables` aisle (150609 items) and the `fresh fruits` aisle (150473 items).

Here's a plot about these aisles:

```{r aisle_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

A table about popular items:

```{r popular_table}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

And a table about the mean hour of each day of the week when Pink Lady Apples and Coffee Ice Cream are ordered:

```{r apples_icecream_table}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
pivot_wider(
  names_from = order_dow,
  values_from = mean_hour) %>% 
  knitr::kable()
```

## Problem 2

First, read the data.

```{r accel_df}
accel_df =
  read_csv("data/accel_data.csv")
```

Let's tidy the data.

```{r tidy}
accel_tidy_df =
  pivot_longer(
    accel_df,
    activity.1:activity.1440,
    names_to = "minute_of_day",
    values_to = "activity_amount"
  ) %>% 
  mutate(minute_of_day = gsub('activity.', '', minute_of_day)) %>%
  mutate(minute_of_day = as.numeric(minute_of_day)) %>% 
  mutate(day = as.factor(day),
         day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
  ) %>% 
  mutate(weekend_weekday = 
           case_when(
             day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "weekday",
             day %in% c("Saturday","Sunday") ~ "weekend"
           )
  )

```

This dataset consists of data from an accelerometer, tracking the activity of a 63-year-old male who was diagnosed with congestive heart failure. The dataset has `r nrow(accel_tidy_df)` rows and `r ncol(accel_tidy_df)` columns. We can see the `activity_amount` for each `minute_of_day` across a five-week period. Each observation is a minute during this time frame, and there are `r nrow(accel_tidy_df)` such observations. Other variables include the `week` number (out of 5 weeks), the `day` of the week, and a variable that tells us whether the activity took place on the weekend or weekday.

Total activity per day:

```{r total_per_day}
activity_day_df =
  accel_tidy_df %>% 
  group_by(day_id) %>% 
  summarize(activity_day = sum(activity_amount))

knitr::kable(activity_day_df)
```

From simply viewing this table, we can see there are a few days when this patient is slightly more or less active, but their daily activity amount is generally within the range of 300000 to 600000.

Let's plot activity over the course of a day (for all 35 days):

```{r plot_activity}
accel_tidy_df %>%
  ggplot(aes(x = minute_of_day, y = activity_amount, color = day_id)) +
  geom_point(alpha = .2) +
  geom_line(alpha = .3) +
  labs(
    title = "Daily activity",
    x = "Minute of Day",
    y = "Amount of activity"
  ) +
  scale_x_continuous(
    breaks = c(60, 180, 300, 420, 540, 660, 780, 900, 1020, 1140, 1260, 1380, 1500),
    labels = c(60, 180, 300, 420, 540, 660, 780, 900, 1020, 1140, 1260, 1380, 1500)
  )
```

Based on this graph, which shows the activity over the course of the day (for all 35 days), we can see some trends pertaining to how active this patient generally is at different times of the day.  Activity tends to be lower late at night and very early in the day, though it doesn't appear he is incredibly active most days during the day. It does appear the patient has a few small bursts of energey during the day, notably around 7:30pm-9:30pm at night.


# Problem 3

Load the data.

```{r read_data}
library(p8105.datasets)
data("ny_noaa")
```

This dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Important variables include the `id` of the weather station, the `date` of each observation, the precipitation, snowfall, snow depth, and maximum and minimum temperatures. All of the observations took place in the state of New York. We should note that there are a lot of missing values.

Let's clean this data.

```{r clean_df}
ny_noaa = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), convert = TRUE)

head(ny_noaa)

ny_noaa_clean =
  na.omit(ny_noaa) %>% 
  mutate(prcp = prcp * .1) %>% 
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin)
         ) %>% 
  mutate(tmax = tmax * .1,
         tmin = tmin * .1
         )

ny_noaa_clean %>% 
  filter(snow > 0) %>%
  pull(snow) %>%
  median()

```

The median snowfall is 25mm.

```{r}
ny_noaa_clean %>%
    mutate(month = month.name[month]) %>%
    filter(month %in% c("January", "July")) %>%
    group_by(year, month, id) %>%
    summarize(avg_max_tmp = mean(tmax)) %>%
    ggplot(aes(x = year, y = avg_max_tmp)) +
    geom_point(alpha = .5) +
    geom_line(alpha = .5) +
    facet_grid(~ month)
```

As expected, we can see the difference in temperatures between January and July, with July being much higher. There's not a pattern of particular note - both months tend to fluctuate from year to year. Both have a few outliers - January has a few particularly low temperatures around 1982 and 1996. July has one particularly low temperature in 1988. 

